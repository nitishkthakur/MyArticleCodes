Rough Code:


import requests
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain.tools import tool

# --- 1. The Corrected Custom Class ---
# We inherit from ChatOllama to get tool use and structured output capabilities
class ProxiedChatOllama(ChatOllama):
    """
    Custom ChatOllama class to support proxies and disable SSL verification.
    This class enables advanced features like tool calling and structured output.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Define your proxy and SSL verification settings here
        self.proxies = {"http": "http://your-proxy-url:port", "https": "https://your-proxy-url:port"}
        self.verify = False

    # The client creation logic is the same, as both Ollama and ChatOllama
    # use this underlying property from a common base class.
    @property
    def _create_client(self):
        """Create the httpx-compatible requests client."""
        # Note: Langchain's Ollama classes use an httpx-like interface.
        # While requests.Session works for basic cases, for full compatibility,
        # especially with async operations, you might need to configure an httpx.Client.
        # However, for synchronous requests, this approach is generally effective.
        session = requests.Session()
        session.proxies = self.proxies
        session.verify = self.verify
        return session

# --- 2. Instantiate the Custom Model ---
# Ensure you are using an Ollama model that is good at following JSON format instructions
# or has been fine-tuned for tool calling (e.g., llama3, phi3, etc.).
llm = ProxiedChatOllama(
    model="llama3",
    base_url="https://your-ollama-server-ip:port",
    format="json" # Important for forcing JSON output when needed
)

# --- 3. Example: Tool Use ---

print("--- Running Tool Use Example ---")

# Define a tool
@tool
def get_current_weather(location: str, unit: str = "fahrenheit") -> str:
    """Get the current weather in a given location."""
    return f"The weather in {location} is 75 degrees {unit} and sunny."

# Bind the tool to the model
llm_with_tools = llm.bind_tools([get_current_weather])

# Create a chain
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "{input}"),
])
chain_with_tools = prompt | llm_with_tools

# Invoke the chain
response = chain_with_tools.invoke({"input": "What is the weather in San Francisco?"})

print("\nModel Response:")
print(response)
print("\nDetected Tool Calls:")
print(response.tool_calls)


# --- 4. Example: Structured Output ---

print("\n\n--- Running Structured Output Example ---")

# Define your desired Pydantic output schema
class Joke(BaseModel):
    """A joke with a setup and a punchline."""
    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline of the joke")
    rating: int = Field(description="A rating of the joke's funniness from 1 to 10")

# Create a new chain for structured output
structured_llm = llm.with_structured_output(Joke)
chain_structured = prompt | structured_llm

# Invoke the chain
joke_response = chain_structured.invoke({"input": "Tell me a great joke about programming."})

print("\nStructured Output Response:")
print(joke_response)
print(f"\nSetup: {joke_response.setup}")
print(f"Punchline: {joke_response.punchline}")
print(f"Rating: {joke_response.rating}")




















































import requests
from langchain_community.llms import Ollama

class ProxiedOllama(Ollama):
    """
    Custom Ollama class to support proxies and disable SSL verification.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.proxies = {"http": "http://your-proxy-url:port", "https": "https://your-proxy-url:port"}
        self.verify = False

    @property
    def _create_client(self):
        """Create the httpx client."""
        return requests.Session(proxies=self.proxies, verify=self.verify)

# Example usage:
llm = ProxiedOllama(
    model="llama2",
    base_url="https://your-ollama-server-ip:port"
)

# Now you can use this 'llm' object in your LangChain and LangGraph workflows.
# For example, with a simple chain:
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

prompt = PromptTemplate(
    input_variables=["topic"],
    template="Tell me a joke about {topic}",
)

chain = LLMChain(llm=llm, prompt=prompt)

response = chain.invoke("a developer")
print(response)



########################
Snippet 2
########################
import httpx
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

# Create custom httpx client with your requirements
custom_client = httpx.Client(
    verify=False,  # Disable SSL verification
    proxies={
        "http://": "http://your-proxy:port",
        "https://": "http://your-proxy:port"
    },
    timeout=30.0
)

# Method 1: Monkey patch the default client
import langchain_community.llms.ollama as ollama_module
ollama_module.httpx.Client = lambda **kwargs: custom_client

# Now use LangChain normally
llm = Ollama(
    model="llama2",
    base_url="https://your-linux-server:11434"
)


############################
Snippet 3
############################
import httpx
import json
from typing import Any, Dict, List, Optional
from langchain_core.llms.base import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun

class CustomOllama(LLM):
    base_url: str = "http://localhost:11434"
    model: str = "llama2"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.client = httpx.Client(
            verify=False,
            proxies={
                "http://": "http://your-proxy:port",
                "https://": "http://your-proxy:port"
            },
            timeout=30.0
        )
    
    @property
    def _llm_type(self) -> str:
        return "custom_ollama"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False
        }
        
        response = self.client.post(
            f"{self.base_url}/api/generate",
            json=payload
        )
        response.raise_for_status()
        
        return response.json()["response"]

# Usage
llm = CustomOllama(
    base_url="https://your-linux-server:11434",
    model="llama2"
)


#################
Snipppet 4
################
import os
import ssl
import urllib3
from langchain_community.llms import Ollama

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Set environment variables
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''
os.environ['HTTP_PROXY'] = 'http://your-proxy:port'
os.environ['HTTPS_PROXY'] = 'http://your-proxy:port'

# Patch SSL context globally (use with caution)
ssl._create_default_https_context = ssl._create_unverified_context

llm = Ollama(
    model="llama2",
    base_url="https://your-linux-server:11434"
)


############# 
Snippet 5 
#############
import httpx
from langchain_openai import ChatOpenAI

# Create custom httpx client
custom_client = httpx.Client(
    verify=False,
    proxies={
        "http://": "http://your-proxy:port",
        "https://": "http://your-proxy:port"
    }
)

# Monkey patch OpenAI's httpx client
import openai
openai.httpx.Client = lambda **kwargs: custom_client
openai.httpx.AsyncClient = lambda **kwargs: httpx.AsyncClient(
    verify=False,
    proxies={
        "http://": "http://your-proxy:port", 
        "https://": "http://your-proxy:port"
    }
)

llm = ChatOpenAI(
    api_key="ollama",  # Ollama doesn't require real API key
    base_url="https://your-linux-server:11434/v1",
    model="llama2"
)


