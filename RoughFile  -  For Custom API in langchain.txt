Rough Code:

import requests
import operator
from typing import TypedDict, Annotated, Sequence

from langchain_community.chat_models import ChatOllama
from langchain_core.messages import BaseMessage, HumanMessage
from langchain.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor

# --- 1. The Custom Proxied LLM Class (Unchanged) ---
# This class is already compatible with LangGraph because it inherits correctly.

class ProxiedChatOllama(ChatOllama):
    """
    Custom ChatOllama class for proxy support and disabled SSL verification.
    Fully compatible with LangChain expressions and LangGraph agents.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # IMPORTANT: Replace with your actual proxy details
        self.proxies = {"http": "http://your-proxy-url:port", "https": "https://your-proxy-url:port"}
        self.verify = False

    @property
    def _create_client(self):
        """Create the httpx-compatible requests client."""
        session = requests.Session()
        session.proxies = self.proxies
        session.verify = self.verify
        return session

# --- 2. Define Tools and the Tool Executor ---
# For this example, we'll create a simple "search" tool.

@tool
def search_tool(query: str) -> str:
    """Searches for information on a given topic and returns a one-sentence summary."""
    print(f"--- Calling Search Tool with query: '{query}' ---")
    # In a real scenario, this would call a search API.
    # We will simulate a result for demonstration.
    if "langgraph" in query.lower():
        return "LangGraph is a library for building stateful, multi-actor applications with LLMs."
    return "This is a simulated search result."

# The ToolExecutor is responsible for calling the correct tool with the given arguments.
tools = [search_tool]
tool_executor = ToolExecutor(tools)

# --- 3. Instantiate the Model and Bind Tools ---
# Ensure your Ollama model is good at tool calling (e.g., llama3, phi3).

llm = ProxiedChatOllama(model="llama3", base_url="https://your-ollama-server-ip:port")
llm_with_tools = llm.bind_tools(tools)

# --- 4. Define the Agent's State ---
# The state is a dictionary that gets passed between the nodes of the graph.

class AgentState(TypedDict):
    # The 'operator.add' line means that messages are appended to this list.
    messages: Annotated[Sequence[BaseMessage], operator.add]

# --- 5. Define the Nodes of the Graph ---

# Node 1: The "brain" of the agent. It calls the LLM.
def call_model(state):
    """Invokes the LLM to decide the next action or respond."""
    messages = state['messages']
    print("--- Calling Model ---")
    response = llm_with_tools.invoke(messages)
    # The response is added to the state.
    return {"messages": [response]}

# Node 2: The "action taker". It executes the tools the LLM decided to call.
def call_tool(state):
    """Executes any tools called by the model and returns the results."""
    last_message = state['messages'][-1]  # Get the latest AI message
    # We check if the model decided to call a tool.
    if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
        return {"messages": []} # No tool calls, do nothing.

    print("--- Executing Tools ---")
    # The ToolExecutor invokes the tools and returns the output.
    tool_results = tool_executor.invoke(last_message)
    return {"messages": tool_results}

# --- 6. Define the Graph's Logic (Conditional Edges) ---

def should_continue(state) -> str:
    """
    Determines the next step: call a tool or end the conversation.
    """
    last_message = state['messages'][-1]
    # If the last message does not have tool calls, we are done.
    if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
        return "end"
    # Otherwise, we need to call the tools.
    else:
        return "continue"

# --- 7. Construct and Compile the Graph ---

# Define a new graph
workflow = StateGraph(AgentState)

# Add the nodes. The first node is the entry point.
workflow.add_node("agent", call_model)
workflow.add_node("action", call_tool)

# Set the entry point for the graph
workflow.set_entry_point("agent")

# Add the conditional edges.
workflow.add_conditional_edges(
    # Start at the 'agent' node
    "agent",
    # The 'should_continue' function decides where to go next
    should_continue,
    {
        # If the decision is "continue", go to the 'action' node
        "continue": "action",
        # If the decision is "end", finish the graph
        "end": END,
    },
)

# Any time we finish the 'action' node (tool execution), we loop back to the 'agent'
# to let the model process the tool's results.
workflow.add_edge("action", "agent")

# Compile the graph into a runnable object.
app = workflow.compile()

# --- 8. Run the Agent ---

# Let's interact with the agent.
inputs = {"messages": [HumanMessage(content="What is langgraph?")]}
for output in app.stream(inputs):
    # stream() yields dictionaries with output from each step
    for key, value in output.items():
        print(f"Output from node '{key}':")
        print("---")
        print(value)
    print("\n---\n")


################ Option 2 ######################
################################################
import requests
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain.tools import tool

# --- 1. The Corrected Custom Class ---
# We inherit from ChatOllama to get tool use and structured output capabilities
class ProxiedChatOllama(ChatOllama):
    """
    Custom ChatOllama class to support proxies and disable SSL verification.
    This class enables advanced features like tool calling and structured output.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Define your proxy and SSL verification settings here
        self.proxies = {"http": "http://your-proxy-url:port", "https": "https://your-proxy-url:port"}
        self.verify = False

    # The client creation logic is the same, as both Ollama and ChatOllama
    # use this underlying property from a common base class.
    @property
    def _create_client(self):
        """Create the httpx-compatible requests client."""
        # Note: Langchain's Ollama classes use an httpx-like interface.
        # While requests.Session works for basic cases, for full compatibility,
        # especially with async operations, you might need to configure an httpx.Client.
        # However, for synchronous requests, this approach is generally effective.
        session = requests.Session()
        session.proxies = self.proxies
        session.verify = self.verify
        return session

# --- 2. Instantiate the Custom Model ---
# Ensure you are using an Ollama model that is good at following JSON format instructions
# or has been fine-tuned for tool calling (e.g., llama3, phi3, etc.).
llm = ProxiedChatOllama(
    model="llama3",
    base_url="https://your-ollama-server-ip:port",
    format="json" # Important for forcing JSON output when needed
)

# --- 3. Example: Tool Use ---

print("--- Running Tool Use Example ---")

# Define a tool
@tool
def get_current_weather(location: str, unit: str = "fahrenheit") -> str:
    """Get the current weather in a given location."""
    return f"The weather in {location} is 75 degrees {unit} and sunny."

# Bind the tool to the model
llm_with_tools = llm.bind_tools([get_current_weather])

# Create a chain
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "{input}"),
])
chain_with_tools = prompt | llm_with_tools

# Invoke the chain
response = chain_with_tools.invoke({"input": "What is the weather in San Francisco?"})

print("\nModel Response:")
print(response)
print("\nDetected Tool Calls:")
print(response.tool_calls)


# --- 4. Example: Structured Output ---

print("\n\n--- Running Structured Output Example ---")

# Define your desired Pydantic output schema
class Joke(BaseModel):
    """A joke with a setup and a punchline."""
    setup: str = Field(description="The setup of the joke")
    punchline: str = Field(description="The punchline of the joke")
    rating: int = Field(description="A rating of the joke's funniness from 1 to 10")

# Create a new chain for structured output
structured_llm = llm.with_structured_output(Joke)
chain_structured = prompt | structured_llm

# Invoke the chain
joke_response = chain_structured.invoke({"input": "Tell me a great joke about programming."})

print("\nStructured Output Response:")
print(joke_response)
print(f"\nSetup: {joke_response.setup}")
print(f"Punchline: {joke_response.punchline}")
print(f"Rating: {joke_response.rating}")




















































import requests
from langchain_community.llms import Ollama

class ProxiedOllama(Ollama):
    """
    Custom Ollama class to support proxies and disable SSL verification.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.proxies = {"http": "http://your-proxy-url:port", "https": "https://your-proxy-url:port"}
        self.verify = False

    @property
    def _create_client(self):
        """Create the httpx client."""
        return requests.Session(proxies=self.proxies, verify=self.verify)

# Example usage:
llm = ProxiedOllama(
    model="llama2",
    base_url="https://your-ollama-server-ip:port"
)

# Now you can use this 'llm' object in your LangChain and LangGraph workflows.
# For example, with a simple chain:
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

prompt = PromptTemplate(
    input_variables=["topic"],
    template="Tell me a joke about {topic}",
)

chain = LLMChain(llm=llm, prompt=prompt)

response = chain.invoke("a developer")
print(response)



########################
Snippet 2
########################
import httpx
from langchain_community.llms import Ollama
from langchain_community.chat_models import ChatOllama

# Create custom httpx client with your requirements
custom_client = httpx.Client(
    verify=False,  # Disable SSL verification
    proxies={
        "http://": "http://your-proxy:port",
        "https://": "http://your-proxy:port"
    },
    timeout=30.0
)

# Method 1: Monkey patch the default client
import langchain_community.llms.ollama as ollama_module
ollama_module.httpx.Client = lambda **kwargs: custom_client

# Now use LangChain normally
llm = Ollama(
    model="llama2",
    base_url="https://your-linux-server:11434"
)


############################
Snippet 3
############################
import httpx
import json
from typing import Any, Dict, List, Optional
from langchain_core.llms.base import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun

class CustomOllama(LLM):
    base_url: str = "http://localhost:11434"
    model: str = "llama2"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.client = httpx.Client(
            verify=False,
            proxies={
                "http://": "http://your-proxy:port",
                "https://": "http://your-proxy:port"
            },
            timeout=30.0
        )
    
    @property
    def _llm_type(self) -> str:
        return "custom_ollama"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False
        }
        
        response = self.client.post(
            f"{self.base_url}/api/generate",
            json=payload
        )
        response.raise_for_status()
        
        return response.json()["response"]

# Usage
llm = CustomOllama(
    base_url="https://your-linux-server:11434",
    model="llama2"
)


#################
Snipppet 4
################
import os
import ssl
import urllib3
from langchain_community.llms import Ollama

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Set environment variables
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''
os.environ['HTTP_PROXY'] = 'http://your-proxy:port'
os.environ['HTTPS_PROXY'] = 'http://your-proxy:port'

# Patch SSL context globally (use with caution)
ssl._create_default_https_context = ssl._create_unverified_context

llm = Ollama(
    model="llama2",
    base_url="https://your-linux-server:11434"
)


############# 
Snippet 5 
#############
import httpx
from langchain_openai import ChatOpenAI

# Create custom httpx client
custom_client = httpx.Client(
    verify=False,
    proxies={
        "http://": "http://your-proxy:port",
        "https://": "http://your-proxy:port"
    }
)

# Monkey patch OpenAI's httpx client
import openai
openai.httpx.Client = lambda **kwargs: custom_client
openai.httpx.AsyncClient = lambda **kwargs: httpx.AsyncClient(
    verify=False,
    proxies={
        "http://": "http://your-proxy:port", 
        "https://": "http://your-proxy:port"
    }
)

llm = ChatOpenAI(
    api_key="ollama",  # Ollama doesn't require real API key
    base_url="https://your-linux-server:11434/v1",
    model="llama2"
)


